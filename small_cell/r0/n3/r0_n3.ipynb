{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import dirname, abspath, join\n",
    "from os import getcwd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "THIS_DIR = getcwd()\n",
    "CLASS_DIR = abspath(join(THIS_DIR, '../../..', 'dsnclasses'))\n",
    "sys.path.append(CLASS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ENOsmall import ENO\n",
    "from CAPMr0 import CAPM\n",
    "from NN3 import Net, DQN\n",
    "from globalvar import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 99:  TOKYO, 2006 \n",
      "Best Average Reward \t=   -0.294\n",
      "Average Reward \t\t=   -0.828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7d81334198>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TRAIN USING data from TOKYO, WAKKANAI and MINAMIDAITO FROM 2005 to 2014\n",
    "dqn = DQN()\n",
    "\n",
    "NO_OF_ITERATIONS = 100\n",
    "best_avg_reward = -1000 #initialize best average reward to very low value\n",
    "PFILENAME = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(8)) #create random filename\n",
    "BFILENAME = \"best\"+PFILENAME + \".pt\" #this file stores the best model\n",
    "TFILENAME = \"terminal\"+PFILENAME + \".pt\" #this file stores the last model\n",
    "\n",
    "avg_reward_rec = [] #record the yearly average rewards over the entire duration of training\n",
    "print('\\nTRAINING IN PROGRESS')\n",
    "\n",
    "for iteration in range(NO_OF_ITERATIONS):\n",
    "    LOCATION = random.choice(['tokyo','wakkanai','minamidaito'])\n",
    "    YEAR = random.choice(np.arange(2005,2015))\n",
    "    capm = CAPM(LOCATION,YEAR,shuffle=True, trainmode=True) #instantiate the CAPM class\n",
    "    capm.eno = ENO(LOCATION,YEAR, shuffle=True, day_balance=True) #instantiate the environment inside the CAPM class\n",
    "    capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "    clear_output()\n",
    "    print('\\nIteration {}:  {}, {} '.format(iteration, LOCATION.upper(), YEAR))\n",
    "    print(\"Best Average Reward \\t= {:8.3f}\".format(best_avg_reward))\n",
    "\n",
    "\n",
    "    s, r, day_end, year_end = capm.reset()\n",
    "    record = np.empty(4) #record for battery, henergy, reward and action\n",
    "\n",
    "    transition_rec = np.zeros((capm.eno.TIME_STEPS, N_STATES * 2 + 2)) #record all the transition in one day\n",
    "\n",
    "    while True:\n",
    "        a = dqn.choose_action(s)\n",
    "        \n",
    "        # present state = [batt, enp, henergy]\n",
    "        record = np.vstack((record, [s[0],s[2],r, a])) # record battery, henergy, reward and action for troubleshooting\n",
    "                                                       # However, we are interested only in the reward\n",
    "\n",
    "        # take action\n",
    "        s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "        temp_transitions = np.hstack((s, [a, r], s_))\n",
    "        transition_rec[capm.eno.hr-1,:] = temp_transitions\n",
    "\n",
    "        if (day_end):\n",
    "            transition_rec[:,5] = r #broadcast reward to all states\n",
    "            decay_factor = [i for i in (LAMBDA**n for n in reversed(range(0, capm.eno.TIME_STEPS)))]\n",
    "            transition_rec[:,5] = transition_rec[:,5] * decay_factor #decay reward proportionately\n",
    "            dqn.store_day_transition(transition_rec)\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "\n",
    "        if (year_end):\n",
    "#             print(\"End of Year\")\n",
    "            break\n",
    "        \n",
    "        # transition to new state\n",
    "        s = s_\n",
    "\n",
    "    record = np.delete(record, 0, 0) #remove the first row which is garbage\n",
    "    reward_rec = record[:,2]\n",
    "    reward_rec = reward_rec[reward_rec != 0] #remove all zero rewards in the middle of the days\n",
    "    print(\"Average Reward \\t\\t= {:8.3f}\".format(np.mean(reward_rec)))\n",
    "    \n",
    "    # Check if reward beats the High Score and possible save it    \n",
    "    if(best_avg_reward < np.mean(reward_rec)):\n",
    "        best_avg_reward = np.mean(reward_rec)\n",
    "        if (iteration > 20): #save the best models only after 20 iterations\n",
    "            print(\"Saving Model\")\n",
    "            torch.save(dqn.eval_net.state_dict(), BFILENAME)\n",
    "\n",
    "    # Log the average reward in avg_reward_rec\n",
    "    avg_reward_rec = np.append(avg_reward_rec, np.mean(reward_rec))\n",
    "    \n",
    "# End of training\n",
    "# Save the last model\n",
    "torch.save(dqn.eval_net.state_dict(), TFILENAME) \n",
    "\n",
    "# Plot the average reward log\n",
    "plt.plot(avg_reward_rec,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BFILENAME -> loads the best model\n",
    "#TFILENAME -> loads the last model\n",
    "MODELFILE = BFILENAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year run test\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best7RZIZ7X8.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-caebf9e1b4ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# load the required model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODELFILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Used: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMODELFILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/eno/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best7RZIZ7X8.pt'"
     ]
    }
   ],
   "source": [
    "#Test the trained model for using greedy policy\n",
    "TEST_LOCATION = 'tokyo'\n",
    "TEST_YEAR = 2015\n",
    "print('\\nYear run test')\n",
    "\n",
    "dqn = DQN()\n",
    "capm = CAPM(TEST_LOCATION,TEST_YEAR, shuffle=False, trainmode=False)\n",
    "capm.eno = ENO(TEST_LOCATION,TEST_YEAR, shuffle=False, day_balance=False) #instantiate the environment inside the CAPM class\n",
    "capm.HMAX = capm.eno.SMAX #maximum power output of solar cell is set in CAPM object using the value in ENO object\n",
    "\n",
    "\n",
    "# load the required model\n",
    "dqn.eval_net.load_state_dict(torch.load(MODELFILE))\n",
    "dqn.eval_net.eval()\n",
    "print('Model Used: ',MODELFILE)\n",
    "\n",
    "s, r, day_end, year_end = capm.reset()\n",
    "yr_test_record = np.empty(4)\n",
    "\n",
    "while True:\n",
    "    a = dqn.choose_greedy_action(s)\n",
    "\n",
    "    #state = [batt, enp, henergy, fcast]\n",
    "    yr_test_record = np.vstack((yr_test_record, [s[0],s[2],r, a])) #record battery, henergy, reward and action\n",
    "\n",
    "    # take action\n",
    "    s_, r, day_end, year_end = capm.step(a)\n",
    "\n",
    "    if year_end:\n",
    "        print(\"End of Test\")\n",
    "        break\n",
    "       \n",
    "    s = s_\n",
    "\n",
    "yr_test_record = np.delete(yr_test_record, 0, 0) #remove the first row which is garbage\n",
    "\n",
    "#Plot the reward and battery for the entire year run\n",
    "title = TEST_LOCATION.upper() + ',' + str(TEST_YEAR)\n",
    "\n",
    "NO_OF_DAYS = capm.eno.NO_OF_DAYS\n",
    "yr_test_reward_rec = yr_test_record[:,2]\n",
    "yr_test_reward_rec = yr_test_reward_rec[yr_test_reward_rec != 0]\n",
    "print('Average Reward for',title, '=', np.mean(yr_test_reward_rec))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(24,10))\n",
    "fig.suptitle(title, fontsize=15)\n",
    "\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.plot(yr_test_reward_rec)\n",
    "ax1.set_title(\"\\n\\nYear Run Reward\")\n",
    "ax1.set_ylim([-3,1])\n",
    "\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.plot(yr_test_record[:,0],'r')\n",
    "ax2.set_title(\"Year Run Battery\")\n",
    "ax2.set_ylim([0,1])\n",
    "plt.sca(ax2)\n",
    "plt.xticks(np.arange(0, NO_OF_DAYS*24, 50*24),np.arange(0,NO_OF_DAYS,50))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the reward and battery for the entire year run on a day by day basis\n",
    "\n",
    "TIME_AXIS = np.arange(0,capm.eno.TIME_STEPS)\n",
    "for DAY in range(0,10):#capm.eno.NO_OF_DAYS):\n",
    "    START = DAY*24\n",
    "    END = START+24\n",
    "    \n",
    "    daytitle = title + ' - DAY ' + str(DAY)\n",
    "    fig = plt.figure(figsize=(16,4))\n",
    "    st = fig.suptitle(daytitle)\n",
    "\n",
    "    ax2 = fig.add_subplot(121)\n",
    "    ax2.plot(yr_test_record[START:END,1],'g')\n",
    "    ax2.set_title(\"HARVESTED ENERGY\")\n",
    "    plt.xlabel(\"Hour\")\n",
    "    ax2.set_ylim([0,1])\n",
    "\n",
    "    #plot battery for year run\n",
    "    ax1 = fig.add_subplot(122)\n",
    "    ax1.plot(TIME_AXIS,yr_test_record[START:END,0],'r') \n",
    "#     ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*capm.BOPT/capm.BMAX,'r--')\n",
    "    ax1.plot(TIME_AXIS, np.ones(capm.eno.TIME_STEPS)*yr_test_record[START,0],'r--')\n",
    "    ax1.text(0.1, 0.2, \"BINIT = %.2f\\n\" %(yr_test_record[START,0]),fontsize=11, ha='left')\n",
    "    ax1.text(0.1, 0.4, \"TENP = %.2f\\n\" %(capm.BOPT/capm.BMAX-yr_test_record[END,0]),fontsize=11, ha='left')\n",
    "    ax1.text(0.1, 0.3, \"BMEAN = %.2f\\n\" %(np.mean(yr_test_record[START:END,0])),fontsize=11, ha='left')\n",
    "\n",
    "\n",
    "\n",
    "    ax1.set_title(\"YEAR RUN TEST\")\n",
    "    if END < (capm.eno.NO_OF_DAYS*capm.eno.TIME_STEPS):\n",
    "        ax1.text(0.1, 0, \"REWARD = %.2f\\n\" %(yr_test_record[END,2]),fontsize=13, ha='left')\n",
    "    plt.xlabel(\"Hour\")\n",
    "    ax1.set_ylabel('Battery', color='r',fontsize=12)\n",
    "    ax1.set_ylim([0,1])\n",
    "\n",
    "    #plot actions for year run\n",
    "    ax1a = ax1.twinx()\n",
    "    ax1a.plot(yr_test_record[START:END,3])\n",
    "    ax1a.set_ylim([0,N_ACTIONS])\n",
    "    ax1a.set_ylabel('Duty Cycle', color='b',fontsize=12)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    st.set_y(0.95)\n",
    "    fig.subplots_adjust(top=0.75)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
